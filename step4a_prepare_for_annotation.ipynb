{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare for Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this notebook, the dev and test dataset are pre-processed to a format that accomodates manual annotation.\n",
    "\n",
    "- After pre-processing, the dataset will have 8 column, namely: 'conversation', 'sent_id', 'token_id', 'token', 'event', 'time', 'place', 'participant'.\n",
    "\n",
    "- The annotators can mark the 'event', 'time', 'place', 'participant' information to annotate the event-related information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsv_add_id(tsv_file):\n",
    "    '''\n",
    "    This function reads a tsv file and adds conversation, sentence, and token IDs to each word in the file.\n",
    "    Conversation ID is added to the first word of each conversation. Sentence ID is added to the first word of each sentence within the same conversation. Token ID is added to each word within the same sentence.\n",
    "    '''\n",
    "    data_with_id = []\n",
    "    conversation = 1\n",
    "\n",
    "    with open(tsv_file, 'r') as file:\n",
    "        next(file)  # Skip the \"utterance\" line\n",
    "        sentence_id = 1  # Initialize sentence_id\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "\n",
    "            if line.endswith('--------------------'):\n",
    "                conversation += 1\n",
    "                data_with_id.append(('', '', '', ''))  # Add an empty line to separate conversations\n",
    "                sentence_id = 1  # Reset sentence_id for new conversation\n",
    "            elif line:  # Check if the line is not empty\n",
    "                # Add a sentence to make the file more readable for annotations\n",
    "                sentence = line.split('\\t')[0]\n",
    "                data_with_id.append((sentence, '', '', ''))\n",
    "\n",
    "                doc = nlp(sentence)  # Use spaCy to process the sentence\n",
    "                token_id = 1\n",
    "                for token in doc:\n",
    "                    data_with_id.append((str(conversation), str(sentence_id), str(token_id), token.text))  # Add conversation, sentence, and token IDs to each word\n",
    "                    token_id += 1  # Increment token_id\n",
    "                sentence_id += 1  # Increment sentence_id\n",
    "                data_with_id.append(('', '', '', ''))  # Add an empty line to separate sentences\n",
    "\n",
    "    return data_with_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_placeholder(data_with_id, output_file):\n",
    "    '''\n",
    "    This function reads a list of tuples with conversation, sentence, and token IDs and adds 12 additional columns with '-' as content to each tuple.\n",
    "    The output is saved to a tsv file.\n",
    "    '''\n",
    "    # Create DataFrame from data_with_id\n",
    "    df = pd.DataFrame(data_with_id, columns=['conversation', 'sent_id', 'token_id', 'token'])\n",
    "    # Add additional columns with '-' as label\n",
    "    df[f'event'] = '-'\n",
    "    df[f'time'] = '-'\n",
    "    df[f'place'] = '-'\n",
    "    df[f'participant'] = '-'\n",
    "\n",
    "\n",
    "    # Save DataFrame to file\n",
    "    df.to_csv(output_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_to_ready_to_annotate(tsv_file, output_file):\n",
    "    '''\n",
    "    This function takes a tsv file and adds conversation, sentence, and token IDs to each word in the file.\n",
    "    The output is saved to a tsv file with 12 additional columns with '-' as content, which will be used for manual annotations.\n",
    "    '''\n",
    "    data_with_id = tsv_add_id(tsv_file)\n",
    "    add_placeholder(data_with_id, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to dev and test files\n",
    "tsv_test = './response_data/resized_files/test.tsv'\n",
    "tsv_dev = './response_data/resized_files/dev.tsv'\n",
    "\n",
    "directory = './response_data/dataset'\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Check if the path exists, if not, create it\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "output_file_test = f\"{directory}/annotation_test.tsv\"\n",
    "output_file_dev = f\"{directory}/annotation_dev.tsv\"\n",
    "\n",
    "# Call the function\n",
    "files_to_ready_to_annotate(tsv_test, output_file_test)\n",
    "files_to_ready_to_annotate(tsv_dev, output_file_dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
