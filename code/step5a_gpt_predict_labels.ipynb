{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-predict Labels\n",
    "This notebook includes the prompt engineering process of predicting event-related information using GPT-4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import itertools\n",
    "from dotenv import load_dotenv\n",
    "import shutil\n",
    "import ast\n",
    "\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up the OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key: \n"
     ]
    }
   ],
   "source": [
    "openai.api_key = '' # Enter OpenAI API key\n",
    "print(\"OpenAI API Key: {}\".format(openai.api_key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prompt GPT to Generate Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt gpt to generate index in the result (Input the index information of each token)\n",
    "\n",
    "def gpt_label_zeroshot(conversations, MODEL=\"gpt-4o\"):\n",
    "    activity_dict = {}\n",
    "\n",
    "    for i, con in enumerate(conversations):\n",
    "            conversation_index = i + 1\n",
    "            print(f\"Processing conversation {conversation_index}/{len(conversations)}\")\n",
    "            # print(con)\n",
    "            query = [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert in healthcare. I will provide you some conversations between\\\n",
    "                 a chatbot and an elderly person. Please extract the information contained in each conversation: \\\n",
    "                'activity index', 'activity', 'participants', 'place', and 'time', and format every activity as a list.\"},\n",
    "                {\"role\": \"system\", \"content\": \"Each token in the conversation is provided with three information:\\\n",
    "                 conversation id, sentence id, and token id.\"},\n",
    "                {\"role\": \"system\", \"content\": \"When extracting the information, please use the words and phrases appeared \\\n",
    "                in the original conversation. Please also indicate the sentence id and token id of the activity information.\"},\n",
    "                {\"role\": \"system\", \"content\": f\"The conversation index for this conversation is {conversation_index}. \\\n",
    "                If the conversation contains more than one activity, generate a list for each activity, using the\\\n",
    "                 same conversation index but different activity indices. For a new activity, increment the activity index by 1.\"},\n",
    "                {\"role\": \"system\", \"content\": \"Generate the activity information only based on the conversation.\\\n",
    "                 Do not use any external information.\"},\n",
    "                {\"role\": \"system\", \"content\": \"If there are no participants, place, or time of the activity mentioned\\\n",
    "                 in the conversation, please mark as 'None' in the output.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Conversation: {con}\"},\n",
    "                {\"role\": \"system\", \"content\": \"Please provide the output in the following JSON format:\\\n",
    "                 [{'activity_index': 1, 'activity': 'activity 1', 'activity_sentence_id': 1, 'activity_token_ids': [1, 2], \\\n",
    "                'participants': 'participant 1', 'participants_sentence_id': 1, 'participants_token_ids': [3], \\\n",
    "                'place': 'place 1', 'place_sentence_id': 2, 'place_token_ids': [5], 'time': 'time 1', 'time_sentence_id': 3, \\\n",
    "                'time_token_ids': [7]}, {...}]. Please provide the output without Markdown code blocks, \\\n",
    "                and do not include the newline marker \\\\n in the output.\"},\n",
    "            ]\n",
    "\n",
    "            response_query = openai.ChatCompletion.create(\n",
    "                model=MODEL,\n",
    "                messages=query,\n",
    "                temperature=0,\n",
    "                max_tokens=1500\n",
    "            )\n",
    "\n",
    "            response_text = response_query.choices[0].message['content']\n",
    "            activity_dict[f\"conversation_{conversation_index}\"] = response_text\n",
    "    return activity_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate BIO Labels\n",
    "Map the results back to the tokens in the conversation, and generate BIO format labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ./response_data/resized_files/test.tsv\n",
      "Processing conversation 1/54\n",
      "Processing conversation 2/54\n",
      "Processing conversation 3/54\n",
      "Processing conversation 4/54\n",
      "Processing conversation 5/54\n",
      "Processing conversation 6/54\n",
      "Processing conversation 7/54\n",
      "Processing conversation 8/54\n",
      "Processing conversation 9/54\n",
      "Processing conversation 10/54\n",
      "Processing conversation 11/54\n",
      "Processing conversation 12/54\n",
      "Processing conversation 13/54\n",
      "Processing conversation 14/54\n",
      "Processing conversation 15/54\n",
      "Processing conversation 16/54\n",
      "Processing conversation 17/54\n",
      "Processing conversation 18/54\n",
      "Processing conversation 19/54\n",
      "Processing conversation 20/54\n",
      "Processing conversation 21/54\n",
      "Processing conversation 22/54\n",
      "Processing conversation 23/54\n",
      "Processing conversation 24/54\n",
      "Processing conversation 25/54\n",
      "Processing conversation 26/54\n",
      "Processing conversation 27/54\n",
      "Processing conversation 28/54\n",
      "Processing conversation 29/54\n",
      "Processing conversation 30/54\n",
      "Processing conversation 31/54\n",
      "Processing conversation 32/54\n",
      "Processing conversation 33/54\n",
      "Processing conversation 34/54\n",
      "Processing conversation 35/54\n",
      "Processing conversation 36/54\n",
      "Processing conversation 37/54\n",
      "Processing conversation 38/54\n",
      "Processing conversation 39/54\n",
      "Processing conversation 40/54\n",
      "Processing conversation 41/54\n",
      "Processing conversation 42/54\n",
      "Processing conversation 43/54\n",
      "Processing conversation 44/54\n",
      "Processing conversation 45/54\n",
      "Processing conversation 46/54\n",
      "Processing conversation 47/54\n",
      "Processing conversation 48/54\n",
      "Processing conversation 49/54\n",
      "Processing conversation 50/54\n",
      "Processing conversation 51/54\n",
      "Processing conversation 52/54\n",
      "Processing conversation 53/54\n",
      "Processing conversation 54/54\n",
      "Processing file: ./response_data/resized_files/mobility_clean_4.tsv\n",
      "Processing conversation 1/51\n",
      "Processing conversation 2/51\n",
      "Processing conversation 3/51\n",
      "Processing conversation 4/51\n",
      "Processing conversation 5/51\n",
      "Processing conversation 6/51\n",
      "Processing conversation 7/51\n",
      "Processing conversation 8/51\n",
      "Processing conversation 9/51\n",
      "Processing conversation 10/51\n",
      "Processing conversation 11/51\n",
      "Processing conversation 12/51\n",
      "Processing conversation 13/51\n",
      "Processing conversation 14/51\n",
      "Processing conversation 15/51\n",
      "Processing conversation 16/51\n",
      "Processing conversation 17/51\n",
      "Processing conversation 18/51\n",
      "Processing conversation 19/51\n",
      "Processing conversation 20/51\n",
      "Processing conversation 21/51\n",
      "Processing conversation 22/51\n",
      "Processing conversation 23/51\n",
      "Processing conversation 24/51\n",
      "Processing conversation 25/51\n",
      "Processing conversation 26/51\n",
      "Processing conversation 27/51\n",
      "Processing conversation 28/51\n",
      "Processing conversation 29/51\n",
      "Processing conversation 30/51\n",
      "Processing conversation 31/51\n",
      "Processing conversation 32/51\n",
      "Processing conversation 33/51\n",
      "Processing conversation 34/51\n",
      "Processing conversation 35/51\n",
      "Processing conversation 36/51\n",
      "Processing conversation 37/51\n",
      "Processing conversation 38/51\n",
      "Processing conversation 39/51\n",
      "Processing conversation 40/51\n",
      "Processing conversation 41/51\n",
      "Processing conversation 42/51\n",
      "Processing conversation 43/51\n",
      "Processing conversation 44/51\n",
      "Processing conversation 45/51\n",
      "Processing conversation 46/51\n",
      "Processing conversation 47/51\n",
      "Processing conversation 48/51\n",
      "Processing conversation 49/51\n",
      "Processing conversation 50/51\n",
      "Processing conversation 51/51\n",
      "Processing file: ./response_data/resized_files/domestic life_clean_1.tsv\n",
      "Processing conversation 1/46\n",
      "Processing conversation 2/46\n",
      "Processing conversation 3/46\n",
      "Processing conversation 4/46\n",
      "Processing conversation 5/46\n",
      "Processing conversation 6/46\n",
      "Processing conversation 7/46\n",
      "Processing conversation 8/46\n",
      "Processing conversation 9/46\n",
      "Processing conversation 10/46\n",
      "Processing conversation 11/46\n",
      "Processing conversation 12/46\n",
      "Processing conversation 13/46\n",
      "Processing conversation 14/46\n",
      "Processing conversation 15/46\n",
      "Processing conversation 16/46\n",
      "Processing conversation 17/46\n",
      "Processing conversation 18/46\n",
      "Processing conversation 19/46\n",
      "Processing conversation 20/46\n",
      "Processing conversation 21/46\n",
      "Processing conversation 22/46\n",
      "Processing conversation 23/46\n",
      "Processing conversation 24/46\n",
      "Processing conversation 25/46\n",
      "Processing conversation 26/46\n",
      "Processing conversation 27/46\n",
      "Processing conversation 28/46\n",
      "Processing conversation 29/46\n",
      "Processing conversation 30/46\n",
      "Processing conversation 31/46\n",
      "Processing conversation 32/46\n",
      "Processing conversation 33/46\n",
      "Processing conversation 34/46\n",
      "Processing conversation 35/46\n",
      "Processing conversation 36/46\n",
      "Processing conversation 37/46\n",
      "Processing conversation 38/46\n",
      "Processing conversation 39/46\n",
      "Processing conversation 40/46\n",
      "Processing conversation 41/46\n",
      "Processing conversation 42/46\n",
      "Processing conversation 43/46\n",
      "Processing conversation 44/46\n",
      "Processing conversation 45/46\n",
      "Processing conversation 46/46\n",
      "Processing file: ./response_data/resized_files/domestic life_clean_3.tsv\n",
      "Processing conversation 1/46\n",
      "Processing conversation 2/46\n",
      "Processing conversation 3/46\n",
      "Processing conversation 4/46\n",
      "Processing conversation 5/46\n",
      "Processing conversation 6/46\n",
      "Processing conversation 7/46\n",
      "Processing conversation 8/46\n",
      "Processing conversation 9/46\n",
      "Processing conversation 10/46\n",
      "Processing conversation 11/46\n",
      "Processing conversation 12/46\n",
      "Processing conversation 13/46\n",
      "Processing conversation 14/46\n",
      "Processing conversation 15/46\n",
      "Processing conversation 16/46\n",
      "Processing conversation 17/46\n",
      "Processing conversation 18/46\n",
      "Processing conversation 19/46\n",
      "Processing conversation 20/46\n",
      "Processing conversation 21/46\n",
      "Processing conversation 22/46\n",
      "Processing conversation 23/46\n",
      "Processing conversation 24/46\n",
      "Processing conversation 25/46\n",
      "Processing conversation 26/46\n",
      "Processing conversation 27/46\n",
      "Processing conversation 28/46\n",
      "Processing conversation 29/46\n",
      "Processing conversation 30/46\n",
      "Processing conversation 31/46\n",
      "Processing conversation 32/46\n",
      "Processing conversation 33/46\n",
      "Processing conversation 34/46\n",
      "Processing conversation 35/46\n",
      "Processing conversation 36/46\n",
      "Processing conversation 37/46\n",
      "Processing conversation 38/46\n",
      "Processing conversation 39/46\n",
      "Processing conversation 40/46\n",
      "Processing conversation 41/46\n",
      "Processing conversation 42/46\n",
      "Processing conversation 43/46\n",
      "Processing conversation 44/46\n",
      "Processing conversation 45/46\n",
      "Processing conversation 46/46\n",
      "Processing file: ./response_data/resized_files/domestic life_clean_2.tsv\n",
      "Processing conversation 1/45\n",
      "Processing conversation 2/45\n",
      "Processing conversation 3/45\n",
      "Processing conversation 4/45\n",
      "Processing conversation 5/45\n",
      "Processing conversation 6/45\n",
      "Processing conversation 7/45\n",
      "Processing conversation 8/45\n",
      "Processing conversation 9/45\n",
      "Processing conversation 10/45\n",
      "Processing conversation 11/45\n",
      "Processing conversation 12/45\n",
      "Processing conversation 13/45\n",
      "Processing conversation 14/45\n",
      "Processing conversation 15/45\n",
      "Processing conversation 16/45\n",
      "Processing conversation 17/45\n",
      "Processing conversation 18/45\n",
      "Processing conversation 19/45\n",
      "Processing conversation 20/45\n",
      "Processing conversation 21/45\n",
      "Processing conversation 22/45\n",
      "Processing conversation 23/45\n",
      "Processing conversation 24/45\n",
      "Processing conversation 25/45\n",
      "Processing conversation 26/45\n",
      "Processing conversation 27/45\n",
      "Processing conversation 28/45\n",
      "Processing conversation 29/45\n",
      "Processing conversation 30/45\n",
      "Processing conversation 31/45\n",
      "Processing conversation 32/45\n",
      "Processing conversation 33/45\n",
      "Processing conversation 34/45\n",
      "Processing conversation 35/45\n",
      "Processing conversation 36/45\n",
      "Processing conversation 37/45\n",
      "Processing conversation 38/45\n",
      "Processing conversation 39/45\n",
      "Processing conversation 40/45\n",
      "Processing conversation 41/45\n",
      "Processing conversation 42/45\n",
      "Processing conversation 43/45\n",
      "Processing conversation 44/45\n",
      "Processing conversation 45/45\n",
      "Processing file: ./response_data/resized_files/domestic life_clean_4.tsv\n",
      "Processing conversation 1/45\n",
      "Processing conversation 2/45\n",
      "Processing conversation 3/45\n",
      "Processing conversation 4/45\n",
      "Processing conversation 5/45\n",
      "Processing conversation 6/45\n",
      "Processing conversation 7/45\n",
      "Processing conversation 8/45\n",
      "Processing conversation 9/45\n",
      "Processing conversation 10/45\n",
      "Processing conversation 11/45\n",
      "Processing conversation 12/45\n",
      "Processing conversation 13/45\n",
      "Processing conversation 14/45\n",
      "Processing conversation 15/45\n",
      "Processing conversation 16/45\n",
      "Processing conversation 17/45\n",
      "Processing conversation 18/45\n",
      "Processing conversation 19/45\n",
      "Processing conversation 20/45\n",
      "Processing conversation 21/45\n",
      "Processing conversation 22/45\n",
      "Processing conversation 23/45\n",
      "Processing conversation 24/45\n",
      "Processing conversation 25/45\n",
      "Processing conversation 26/45\n",
      "Processing conversation 27/45\n",
      "Processing conversation 28/45\n",
      "Processing conversation 29/45\n",
      "Processing conversation 30/45\n",
      "Processing conversation 31/45\n",
      "Processing conversation 32/45\n",
      "Processing conversation 33/45\n",
      "Processing conversation 34/45\n",
      "Processing conversation 35/45\n",
      "Processing conversation 36/45\n",
      "Processing conversation 37/45\n",
      "Processing conversation 38/45\n",
      "Processing conversation 39/45\n",
      "Processing conversation 40/45\n",
      "Processing conversation 41/45\n",
      "Processing conversation 42/45\n",
      "Processing conversation 43/45\n",
      "Processing conversation 44/45\n",
      "Processing conversation 45/45\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"../response_data/files_with_id\"):\n",
    "    os.makedirs(\"../response_data/files_with_id\")\n",
    "\n",
    "\n",
    " # Process all the files\n",
    "for file in glob.glob(\"../response_data/resized_files/*.tsv\"):\n",
    "    print(f\"Processing file: {file}\")\n",
    "    data_with_id = utils.tsv_add_id(file)\n",
    "\n",
    "    \n",
    "    output_file = f\"../response_data/files_with_id/{os.path.basename(file)}\"\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data_with_id, columns=['conversation', 'sent_id', 'token_id', 'token'])\n",
    "    df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "    conversation_list = utils.extract_conversations(output_file)\n",
    "\n",
    "    activity_dict = gpt_label_zeroshot(conversation_list, MODEL = \"gpt-4o\")\n",
    "\n",
    "    # Remove empty rows from the DataFrame\n",
    "    df = df[df['conversation'] != '']\n",
    "    # Add a label column to the DataFrame, and initialize it with 'O'\n",
    "    df['event1']= 'O'\n",
    "    df['event2']= 'O'\n",
    "    df['event3']= 'O'\n",
    "    df['event4']= 'O'\n",
    "    df['event5']= 'O'\n",
    "\n",
    "\n",
    "    df = utils.get_label(df, activity_dict)\n",
    "    output_file = f\"../response_data/dataset/{os.path.basename(file).replace('.tsv', '_gpt_4o_labelled.tsv')}\"\n",
    "    df.to_csv(output_file, sep='\\t', index=False)\n",
    "    \n",
    "    # save the file to jsonl (when input as conversation)\n",
    "    utils.tsv_to_jsonl(output_file, output_file.split('/')[-1].split('_')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to give BERT sentence by sentence as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset directories\n",
    "\n",
    "if not os.path.exists(\"../response_data/jsonl_for_finetune\"):\n",
    "    os.makedirs(\"../response_data/jsonl_for_finetune\")\n",
    "\n",
    "tsv_dir = '../response_data/dataset/'\n",
    "jsonl_dir = '../response_data/jsonl_for_finetune'\n",
    "\n",
    "# Convert TSV files to JSONL files\n",
    "utils.convert_tsv_to_jsonl(tsv_dir, jsonl_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combine Training Set Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train JSONL files: ['./response_data/jsonl_for_finetune/self-care_clean_1_gpt_4o_labelled.jsonl', './response_data/jsonl_for_finetune/self-care_clean_3_gpt_4o_labelled.jsonl', './response_data/jsonl_for_finetune/domestic life_clean_4_gpt_4o_labelled.jsonl', './response_data/jsonl_for_finetune/domestic life_clean_2_gpt_4o_labelled.jsonl', './response_data/jsonl_for_finetune/mobility_clean_1_gpt_4o_labelled.jsonl', './response_data/jsonl_for_finetune/mobility_clean_3_gpt_4o_labelled.jsonl', './response_data/jsonl_for_finetune/mobility_clean_4_gpt_4o_labelled.jsonl', './response_data/jsonl_for_finetune/self-care_clean_2_gpt_4o_labelled.jsonl', './response_data/jsonl_for_finetune/self-care_clean_4_gpt_4o_labelled.jsonl', './response_data/jsonl_for_finetune/domestic life_clean_3_gpt_4o_labelled.jsonl', './response_data/jsonl_for_finetune/mobility_clean_2_gpt_4o_labelled.jsonl', './response_data/jsonl_for_finetune/domestic life_clean_1_gpt_4o_labelled.jsonl']\n"
     ]
    }
   ],
   "source": [
    "# Dataset directories\n",
    "dataset_directory = \"../response_data/jsonl_for_finetune/\"\n",
    "\n",
    "# Merged file names\n",
    "train_merged_file_name =  \"../response_data/dataset/train.jsonl\"\n",
    "\n",
    "# Delete train.jsonl file if it exists\n",
    "if os.path.exists(train_merged_file_name):\n",
    "    os.remove(train_merged_file_name)\n",
    "    print(\"Deleted existing train.jsonl file.\")\n",
    "\n",
    "\n",
    "# Initialize lists to store train and test JSONL files\n",
    "train_jsonl_files = []\n",
    "\n",
    "# Get all JSONL files in the train directory\n",
    "for file in os.listdir(dataset_directory):\n",
    "    if file.endswith(\".jsonl\") and \"test\" not in file and \"dev\" not in file:\n",
    "        train_jsonl_files.append(os.path.join(dataset_directory, file))\n",
    "\n",
    "print(f\"Train JSONL files: {train_jsonl_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Train Files\n",
    "with open(train_merged_file_name, \"w\", encoding=\"utf-8\") as train_merged_file:\n",
    "    # Iterate over each JSONL file in train set\n",
    "    for file_name in train_jsonl_files:\n",
    "        # Open the current file in train set\n",
    "        with open(file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "            # Read the content of the current file line by line and write it to the new file for train set\n",
    "            for line in file:\n",
    "                train_merged_file.write(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra: Check Ambiguity: Overlapped Spans\n",
    "\n",
    "Check if there are any events that share the same tokens for different activity information.  \n",
    "   \n",
    "E.g. A token is labelled as \"time\" for both \"event 1\" and \"event 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed file: self-care_clean_2_gpt_4o_labelled.tsv, No tokens appear in more than one events found\n",
      "Processed file: dev_gpt_4o_labelled.tsv, No tokens appear in more than one events found\n",
      "Processed file: self-care_clean_3_gpt_4o_labelled.tsv, No tokens appear in more than one events found\n",
      "Processed file: self-care_clean_1_gpt_4o_labelled.tsv, No tokens appear in more than one events found\n",
      "Processed file: test_gpt_4o_labelled.tsv, No tokens appear in more than one events found\n",
      "Processed file: self-care_clean_4_gpt_4o_labelled.tsv, No tokens appear in more than one events found\n",
      "Processed file: domestic life_clean_4_gpt_4o_labelled.tsv, No tokens appear in more than one events found\n",
      "Processed file: mobility_clean_1_gpt_4o_labelled.tsv, No tokens appear in more than one events found\n",
      "Processed file: mobility_clean_3_gpt_4o_labelled.tsv, No tokens appear in more than one events found\n",
      "Processed file: mobility_clean_2_gpt_4o_labelled.tsv, No tokens appear in more than one events found\n",
      "Processed file: mobility_clean_4_gpt_4o_labelled.tsv, No tokens appear in more than one events found\n",
      "Processed file: domestic life_clean_1_gpt_4o_labelled.tsv, No tokens appear in more than one events found\n",
      "Processed file: domestic life_clean_3_gpt_4o_labelled.tsv, No tokens appear in more than one events found\n",
      "Processed file: domestic life_clean_2_gpt_4o_labelled.tsv, No tokens appear in more than one events found\n"
     ]
    }
   ],
   "source": [
    "# Define the directory path\n",
    "directory = '../response_data/dataset/'\n",
    "\n",
    "\n",
    "# Iterate through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.tsv') and \"annotation\" not in filename:\n",
    "        # Construct the file path\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        # Read the TSV file\n",
    "        df = pd.read_csv(filepath, sep='\\t')\n",
    "        \n",
    "        # Flag to track if there are rows with more than one non-'O' and non-NaN event columns\n",
    "        found_non_O_non_NaN = False\n",
    "        \n",
    "        # Check if there are more than one non-'O' and non-NaN event columns for each row\n",
    "        for index, row in df.iterrows():\n",
    "            # Get the columns starting with 'event'\n",
    "            event_columns = [col for col in df.columns if col.startswith('event')]\n",
    "            \n",
    "            # Calculate the count of non-'O' and non-NaN events\n",
    "            non_O_count = sum(row[column] != 'O' and not pd.isna(row[column]) for column in event_columns)\n",
    "            \n",
    "            # If the count of non-'O' and non-NaN events is greater than 1, print the corresponding token column\n",
    "            if non_O_count > 1:\n",
    "                found_non_O_non_NaN = True\n",
    "                print(f'Processed file: {filename}, Token: {row[\"token\"]}')\n",
    "        \n",
    "        # If no rows with more than one non-'O' and non-NaN event columns were found, print a message\n",
    "        if not found_non_O_non_NaN:\n",
    "            print(f'Processed file: {filename}, No tokens appear in more than one events found')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
