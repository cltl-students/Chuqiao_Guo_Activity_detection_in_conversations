{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule-based System\n",
    "\n",
    "- This notebook includes the process of defining rules to detect event-related tokens using dependency parser and NER processor of SpaCy.\n",
    "- It outputs the predictions of thr rule-based system in this dir: 'response_data/dataset/rule-based/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# Load spaCy model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the text of all children of a token\n",
    "def get_all_children_text(token):\n",
    "    \"\"\"\n",
    "    Recursively get the text of the token and its children.\n",
    "    \"\"\"\n",
    "    children_texts = [token.text]\n",
    "    for child in token.children:\n",
    "        children_texts.append(get_all_children_text(child))\n",
    "    return ' '.join(children_texts)\n",
    "\n",
    "def get_all_children_ids(token):\n",
    "    \"\"\"\n",
    "    Recursively get the token IDs of the token and its children.\n",
    "    \"\"\"\n",
    "    token_ids = [token.i]\n",
    "    for child in token.children:\n",
    "        token_ids.extend(get_all_children_ids(child))\n",
    "    return token_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information from sentences\n",
    "def is_valid_participant(chunk):\n",
    "    if chunk.text.lower() in ['it', 'that', 'this', 'there', 'here']:\n",
    "        return False\n",
    "    if any(token.text.lower() in ['some', 'any', 'every'] for token in chunk):\n",
    "        return False\n",
    "    if any(token.pos_ == 'ADJ' for token in chunk):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def extract_information(sentence, sentence_id):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    events = []\n",
    "    times = []\n",
    "    places = []\n",
    "    participants = []\n",
    "    \n",
    "    for token in doc:\n",
    "        # Time\n",
    "        if token.dep_ == 'tmod' or token.ent_type_ in ['DATE', 'TIME']:  # Date expressions\n",
    "            time_phrase = token.text\n",
    "            time_token_ids = [token.i]\n",
    "            for child in token.children:\n",
    "                if child.dep_ in ['det', 'poss']:\n",
    "                    time_phrase = child.text + \" \" + time_phrase\n",
    "                    time_token_ids.insert(0, child.i)\n",
    "            times.append({'time': time_phrase, 'time_sentence_id': sentence_id, 'time_token_ids': time_token_ids})\n",
    "\n",
    "\n",
    "        # Place\n",
    "        location_prepositions = {\"in\", \"on\", \"at\", \"near\", \"by\"}\n",
    "        if token.pos_ == 'ADP' and token.text in location_prepositions:  # Prepositions indicating location\n",
    "            place_phrase = get_all_children_text(token)\n",
    "            place_token_ids = get_all_children_ids(token)\n",
    "            places.append({'place': place_phrase, 'place_sentence_id': sentence_id, 'place_token_ids': place_token_ids})\n",
    "\n",
    "\n",
    "        if token.ent_type_ in ['GPE', 'LOC']:  # Geopolitical entities\n",
    "            places.append({'place': token.text, 'place_sentence_id': sentence_id, 'place_token_ids': [token.i]})\n",
    "\n",
    "        # Event\n",
    "        if token.pos_ == 'VERB':  # Identify verbs\n",
    "            verb_phrase = token.text\n",
    "            verb_token_ids = [token.i]\n",
    "            for child in token.children:\n",
    "                if child.dep_ == 'dobj':  # Direct object\n",
    "                    verb_phrase += \" \" + child.text\n",
    "                    verb_token_ids.append(child.i)\n",
    "                    for subchild in child.children:\n",
    "                        if subchild.dep_ in ['poss', 'det']:\n",
    "                            verb_phrase = subchild.text + \" \" + verb_phrase\n",
    "                            verb_token_ids.append(subchild.i)\n",
    "                    break\n",
    "            events.append({'event': verb_phrase, 'event_sentence_id': sentence_id, 'event_token_ids': verb_token_ids})\n",
    "        \n",
    "\n",
    "    \n",
    "    # Combine consecutive time entries\n",
    "    if times:\n",
    "        combined_times = {'time': '', 'time_sentence_id': sentence_id, 'time_token_ids': []}\n",
    "        for t in times:\n",
    "            if combined_times['time']:\n",
    "                combined_times['time'] += ' ' + t['time']\n",
    "            else:\n",
    "                combined_times['time'] = t['time']\n",
    "            combined_times['time_token_ids'].extend(t['time_token_ids'])\n",
    "        times = [combined_times]\n",
    "    \n",
    "    # Participants\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if (chunk.root.dep_ in ('nsubj') and chunk.root.head.pos_ == 'VERB' and is_valid_participant(chunk)) or \\\n",
    "           (chunk.root.ent_type_ == 'PERSON'):\n",
    "            participants.append({'participants': chunk.text, 'participants_sentence_id': sentence_id, 'participants_token_ids': [token.i for token in chunk]})\n",
    "    \n",
    "    return {\n",
    "        'events': events,\n",
    "        'times': times,\n",
    "        'places': places,\n",
    "        'participants': participants\n",
    "    }\n",
    "\n",
    "\n",
    "def rule_based_extraction(sentences):\n",
    "\n",
    "    # Process each sentence and aggregate information\n",
    "    results = []\n",
    "    for sentence_id, sentence in enumerate(sentences):\n",
    "        if sentence:\n",
    "            info = extract_information(sentence, sentence_id)\n",
    "            results.append(info)\n",
    "\n",
    "    # Flatten results to get a single JSON structure\n",
    "    output = []\n",
    "    for result in results:\n",
    "        for event in result['events']:\n",
    "            entry = {\n",
    "                'event': event['event'],\n",
    "                'event_sentence_id': event['event_sentence_id'],\n",
    "                'event_token_ids': event['event_token_ids'],\n",
    "                'times': result['times'],\n",
    "                'places':result['places'],\n",
    "                'participants': result['participants']\n",
    "            }\n",
    "            output.append(entry)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "\n",
    "def annotate(df, annotations, conversation_id):\n",
    "    events_by_sentence = defaultdict(list)\n",
    "    participants_by_sentence = defaultdict(list)\n",
    "    places_by_sentence = defaultdict(list)\n",
    "    times_by_sentence = defaultdict(list)\n",
    "\n",
    "    # Conbine each argument with the same sentence ID\n",
    "    for ann in annotations:\n",
    "        sentence_id = ann['event_sentence_id'] + 1  # convert index from 0-based to 1-based\n",
    "        for token_id in ann['event_token_ids']:\n",
    "            events_by_sentence[sentence_id].append(token_id + 1)  # convert index from 0-based to 1-based\n",
    "\n",
    "        for participant in ann.get('participants', []):\n",
    "            sentence_id = participant['participants_sentence_id'] + 1 # convert index from 0-based to 1-based\n",
    "            for token_id in participant['participants_token_ids']:\n",
    "                participants_by_sentence[sentence_id].append(token_id + 1)  # convert index from 0-based to 1-based\n",
    "\n",
    "        for place in ann.get('places', []):\n",
    "            sentence_id = place['place_sentence_id'] + 1 # convert index from 0-based to 1-based\n",
    "            for token_id in place['place_token_ids']:\n",
    "                places_by_sentence[sentence_id].append(token_id + 1)  # convert index from 0-based to 1-based\n",
    "\n",
    "        for time in ann.get('times', []):\n",
    "            sentence_id = time['time_sentence_id'] + 1 # convert index from 0-based to 1-based\n",
    "            for token_id in time['time_token_ids']:\n",
    "                times_by_sentence[sentence_id].append(token_id + 1)  # convert index from 0-based to 1-based\n",
    "\n",
    "\n",
    "    # Get the whole event-related information for each sentence\n",
    "    def apply_labels(df, items_by_sentence, label_prefix):\n",
    "        for sentence_id, token_ids in items_by_sentence.items():\n",
    "            token_ids.sort() \n",
    "            previous_token_id = -2  \n",
    "            for i, token_id in enumerate(token_ids):\n",
    "                # If the current token ID is consecutive to the previous one, it is an inside label\n",
    "                label = 'I-' + label_prefix if token_id == previous_token_id + 1 else 'B-' + label_prefix\n",
    "                # print(f\"Token ID: {token_id}, Label: {label} (Previous Token ID: {previous_token_id})\") \n",
    "                \n",
    "                df.loc[\n",
    "                    (df['conversation'] == conversation_id) & \n",
    "                    (df['sent_id'] == sentence_id) & \n",
    "                    (df['token_id'] == token_id), \n",
    "                    'label'\n",
    "                ] = label\n",
    "                previous_token_id = token_id  #Update the previous token ID\n",
    "\n",
    "    apply_labels(df, events_by_sentence, 'event')\n",
    "    apply_labels(df, participants_by_sentence, 'participant')\n",
    "    apply_labels(df, places_by_sentence, 'place')\n",
    "    apply_labels(df, times_by_sentence, 'time')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Iterate over the files, excluding the dev and test sets\n",
    "for file_path in glob.glob(\"./response_data/resized_files/*.tsv\"):\n",
    "    if \"dev\" in file_path or \"test\" in file_path:\n",
    "        with open(file_path, 'r') as file:\n",
    "\n",
    "            # Read the TSV file, skip empty lines\n",
    "            tsv_file_path = f\"./response_data/files_with_id/{file_path.split('/')[-1]}\"\n",
    "            df = pd.read_csv(tsv_file_path, delimiter='\\t', skip_blank_lines=True)\n",
    "            df = df.dropna(subset=['token'])\n",
    "\n",
    "            # Initialise the label column to 'O'\n",
    "            df['label'] = 'O'  \n",
    "\n",
    "            next(file)  # Skip the header\n",
    "\n",
    "            content = file.read()\n",
    "            conversations = content.split('--------------------')\n",
    "            \n",
    "            # Interate over the conversations\n",
    "            conversation_id = 1\n",
    "            for conversation in conversations:\n",
    "\n",
    "                conversation = conversation.strip()\n",
    "                if conversation:\n",
    "                    sentences = conversation.strip().split('\\n')\n",
    "                    if len(sentences) > 1:\n",
    "                        # Extract information from conversation\n",
    "                        results = rule_based_extraction(sentences)\n",
    "\n",
    "                        # Annotate the data\n",
    "                        df = annotate(df, results, conversation_id)\n",
    "                        conversation_id += 1\n",
    "\n",
    "            # Format the columns as integers\n",
    "            df['conversation'] = df['conversation'].astype(int)\n",
    "            df['sent_id'] = df['sent_id'].astype(int)\n",
    "            df['token_id'] = df['token_id'].astype(int)\n",
    "\n",
    "            # save the annotated data\n",
    "            rule_based_dir = 'response_data/dataset/rule-based/'\n",
    "            if not os.path.exists(rule_based_dir):\n",
    "                os.makedirs(rule_based_dir)\n",
    "\n",
    "            output_tsv_file_path = rule_based_dir + 'improved_' + file_path.split('/')[-1]\n",
    "            df.to_csv(output_tsv_file_path, sep='\\t', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
