{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resizing Files\n",
    "- This notebook resizes each category of generated data into four smaller files for better processed by GPT\n",
    "- It also include the train-dev-test split process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Resizing Files\n",
    "\n",
    "The resizing step mainly uses code from the following GitHub repository: [ICF-activities-classifier](https://github.com/cltl-students/ICF-activities-classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_size(lst, sep = \"-\"*20):\n",
    "    \"\"\"\"\"\"\n",
    "    match = [x for x in lst if x == sep]\n",
    "    num = len(match)\n",
    "    count = 0\n",
    "    lst_1 = []\n",
    "    lst_2 = []\n",
    "    lst_3 = []\n",
    "    lst_4 = []\n",
    "    for e in lst:\n",
    "        if count < num/4:\n",
    "            if e == \"-\"*20:\n",
    "                count += 1 \n",
    "            lst_1.append(e)\n",
    "        elif num/2 > count >= num/4:\n",
    "            if e == \"-\"*20:\n",
    "                count += 1 \n",
    "            lst_2.append(e)\n",
    "        elif (num/4)*3 > count >= num/2:\n",
    "            if e == \"-\"*20:\n",
    "                count += 1 \n",
    "            lst_3.append(e)\n",
    "        else:\n",
    "            if e == \"-\"*20:\n",
    "                count += 1 \n",
    "            lst_4.append(e)\n",
    "    print(num == count)\n",
    "    return lst_1, lst_2, lst_3, lst_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dir = './response_data/'\n",
    "files = glob.glob(f\"{dir}clean_conversations/*.tsv\")\n",
    "\n",
    "if not os.path.exists(f\"{dir}resized_files\"):\n",
    "    os.makedirs(f\"{dir}resized_files\")\n",
    "\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, sep=\"\\t\")\n",
    "    lst = df[\"utterances\"].tolist()\n",
    "    l1, l2, l3, l4 = cut_size(lst)\n",
    "    df_1 = pd.DataFrame({\"utterances\":l1})\n",
    "    df_2 = pd.DataFrame({\"utterances\": l2})\n",
    "    df_3 = pd.DataFrame({\"utterances\": l3})\n",
    "    df_4 = pd.DataFrame({\"utterances\": l4})\n",
    "    fileparts = f.split(\"/\")\n",
    "    name = fileparts[-1].rstrip(\".tsv\")\n",
    "    l1_path = f\"{dir}resized_files/\"+ name + \"_1\" + \".tsv\"\n",
    "    l2_path = f\"{dir}resized_files/\"+ name + \"_2\" + \".tsv\"\n",
    "    l3_path = f\"{dir}resized_files/\"+ name + \"_3\" + \".tsv\"\n",
    "    l4_path = f\"{dir}resized_files/\"+ name + \"_4\" + \".tsv\"\n",
    "    df_1.to_csv(l1_path, sep=\"\\t\")\n",
    "    df_2.to_csv(l2_path, sep=\"\\t\")\n",
    "    df_3.to_csv(l3_path, sep=\"\\t\")\n",
    "    df_4.to_csv(l4_path, sep=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_size(lst, sep=\"-\"*20):\n",
    "    match = [x for x in lst if x == sep]\n",
    "    num = len(match)\n",
    "    count = 0\n",
    "    lst_1, lst_2, lst_3, lst_4 = [], [], [], []\n",
    "\n",
    "    for e in lst:\n",
    "        if count < num/4:\n",
    "            if e == sep:\n",
    "                count += 1\n",
    "            lst_1.append(e)\n",
    "        elif num/2 > count >= num/4:\n",
    "            if e == sep:\n",
    "                count += 1\n",
    "            lst_2.append(e)\n",
    "        elif (num/4)*3 > count >= num/2:\n",
    "            if e == sep:\n",
    "                count += 1\n",
    "            lst_3.append(e)\n",
    "        else:\n",
    "            if e == sep:\n",
    "                count += 1\n",
    "            lst_4.append(e)\n",
    "    return lst_1, lst_2, lst_3, lst_4\n",
    "\n",
    "def extract_dev_test(lst, sep=\"-\"*20, num_conversations=18):\n",
    "    '''\n",
    "    This function extracts a number of conversations from a list of conversations, which will be used as dev+test data.\n",
    "    It takes a list of conversations, a separator, and the number of conversations to extract.\n",
    "    It returns the selected conversations and the remaining conversations.\n",
    "    '''\n",
    "    conversations = []\n",
    "    current_conversation = []\n",
    "    for line in lst:\n",
    "        current_conversation.append(line)\n",
    "        if line == sep:\n",
    "            conversations.append(current_conversation)\n",
    "            current_conversation = []\n",
    "    if current_conversation:\n",
    "        conversations.append(current_conversation)\n",
    "\n",
    "    training_set = random.sample(conversations, num_conversations)\n",
    "    dev_test_set = [c for c in conversations if c not in training_set]\n",
    "\n",
    "    trainingset = [item for sublist in training_set for item in sublist]\n",
    "    devset = [item for sublist in dev_test_set for item in sublist]\n",
    "\n",
    "    return trainingset, devset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train-dev-test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly pick 18 conversations from each category to use as test data, 18 as dev data, and the rest as training data\n",
    "dir = './response_data/'\n",
    "files = glob.glob(f\"{dir}clean_conversations/*.tsv\")\n",
    "\n",
    "if not os.path.exists(f\"{dir}resized_files\"):\n",
    "    os.makedirs(f\"{dir}resized_files\")\n",
    "\n",
    "test_conversations = []\n",
    "dev_conversations = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, sep=\"\\t\")\n",
    "    lst = df[\"utterances\"].tolist()\n",
    "\n",
    "    testset, trainingset = extract_dev_test(lst)\n",
    "    test_conversations.extend(testset)\n",
    "\n",
    "    devset, trainingset = extract_dev_test(trainingset)\n",
    "    dev_conversations.extend(devset)\n",
    "\n",
    "    l1, l2, l3, l4 = cut_size(trainingset)\n",
    "\n",
    "    df_1 = pd.DataFrame({\"utterances\": l1})\n",
    "    df_2 = pd.DataFrame({\"utterances\": l2})\n",
    "    df_3 = pd.DataFrame({\"utterances\": l3})\n",
    "    df_4 = pd.DataFrame({\"utterances\": l4})\n",
    "\n",
    "    fileparts = f.split(\"/\")\n",
    "    name = fileparts[-1].rstrip(\".tsv\")\n",
    "    l1_path = f\"{dir}resized_files/\" + name + \"_1\" + \".tsv\"\n",
    "    l2_path = f\"{dir}resized_files/\" + name + \"_2\" + \".tsv\"\n",
    "    l3_path = f\"{dir}resized_files/\" + name + \"_3\" + \".tsv\"\n",
    "    l4_path = f\"{dir}resized_files/\" + name + \"_4\" + \".tsv\"\n",
    "\n",
    "    df_1.to_csv(l1_path, sep=\"\\t\", index=False)\n",
    "    df_2.to_csv(l2_path, sep=\"\\t\", index=False)\n",
    "    df_3.to_csv(l3_path, sep=\"\\t\", index=False)\n",
    "    df_4.to_csv(l4_path, sep=\"\\t\", index=False)\n",
    "\n",
    "# Save the selected conversations to test.tsv\n",
    "test_df = pd.DataFrame({\"utterances\": test_conversations})\n",
    "test_df.to_csv(f\"{dir}resized_files/test.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# Save the selected conversations to dev.tsv\n",
    "dev_df = pd.DataFrame({\"utterances\": dev_conversations})\n",
    "dev_df.to_csv(f\"{dir}resized_files/dev.tsv\", sep=\"\\t\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
